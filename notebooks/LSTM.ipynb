{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "import math\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = pd.read_csv(\"../data/dataset_mr/Trainset_complete.csv\", sep=';') \n",
    "test_sentences = pd.read_csv(\"../data/dataset_mr/Validationset.csv\", sep=';') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7096</td>\n",
       "      <td>7108</td>\n",
       "      <td>131841</td>\n",
       "      <td>Michele is a such a brainless flibbertigibbet ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6876</td>\n",
       "      <td>6887</td>\n",
       "      <td>128029</td>\n",
       "      <td>The question hanging over The Time Machine is ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7310</td>\n",
       "      <td>7322</td>\n",
       "      <td>135561</td>\n",
       "      <td>Worse than ` Silence of the Lambs ' better tha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7002</td>\n",
       "      <td>7014</td>\n",
       "      <td>130241</td>\n",
       "      <td>Ninety minutes of Viva Castro !</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6629</td>\n",
       "      <td>6639</td>\n",
       "      <td>123623</td>\n",
       "      <td>Dong shows how intolerance has the power to de...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6982</td>\n",
       "      <td>6994</td>\n",
       "      <td>129911</td>\n",
       "      <td>Graham Greene 's novel of colonialism and empi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6441</td>\n",
       "      <td>6451</td>\n",
       "      <td>120640</td>\n",
       "      <td>It grabs you in the dark and shakes you vigoro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7044</td>\n",
       "      <td>7056</td>\n",
       "      <td>130906</td>\n",
       "      <td>Mushes the college-friends genre -LRB- The Big...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6544</td>\n",
       "      <td>6554</td>\n",
       "      <td>122262</td>\n",
       "      <td>-LRB- H -RRB- ad I suffered and bled on the ha...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6489</td>\n",
       "      <td>6499</td>\n",
       "      <td>121347</td>\n",
       "      <td>Nettelbeck ... has a pleasing way with a metap...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  SentenceId  PhraseId  \\\n",
       "0        7096        7108    131841   \n",
       "1        6876        6887    128029   \n",
       "2        7310        7322    135561   \n",
       "3        7002        7014    130241   \n",
       "4        6629        6639    123623   \n",
       "5        6982        6994    129911   \n",
       "6        6441        6451    120640   \n",
       "7        7044        7056    130906   \n",
       "8        6544        6554    122262   \n",
       "9        6489        6499    121347   \n",
       "\n",
       "                                              Phrase  Sentiment  \n",
       "0  Michele is a such a brainless flibbertigibbet ...         -1  \n",
       "1  The question hanging over The Time Machine is ...          0  \n",
       "2  Worse than ` Silence of the Lambs ' better tha...          0  \n",
       "3                    Ninety minutes of Viva Castro !          0  \n",
       "4  Dong shows how intolerance has the power to de...          0  \n",
       "5  Graham Greene 's novel of colonialism and empi...          1  \n",
       "6  It grabs you in the dark and shakes you vigoro...          1  \n",
       "7  Mushes the college-friends genre -LRB- The Big...          0  \n",
       "8  -LRB- H -RRB- ad I suffered and bled on the ha...         -1  \n",
       "9  Nettelbeck ... has a pleasing way with a metap...          1  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of each review :  18.783382789317507\n",
      "Percentage of reviews with positive sentiment is 33.33333333333333%\n",
      "Percentage of reviews with negative sentiment is 33.33333333333333%\n",
      "Percentage of reviews with neutral sentiment is 33.33333333333333%\n"
     ]
    }
   ],
   "source": [
    "s = 0.0\n",
    "for i in train_sentences['Phrase']:\n",
    "    word_list = i.split()\n",
    "    s = s + len(word_list)\n",
    "print(\"Average length of each review : \",s/train_sentences.shape[0])\n",
    "pos = 0\n",
    "neg = 0\n",
    "for i in range(train_sentences.shape[0]):\n",
    "    if train_sentences.iloc[i]['Sentiment'] == 1:\n",
    "        pos = pos + 1\n",
    "    elif train_sentences.iloc[i]['Sentiment'] == -1:\n",
    "        neg = neg + 1\n",
    "    \n",
    "neu = train_sentences.shape[0]-pos-neg\n",
    "print(\"Percentage of reviews with positive sentiment is \"+str(pos/train_sentences.shape[0]*100)+\"%\")\n",
    "print(\"Percentage of reviews with negative sentiment is \"+str(neg/train_sentences.shape[0]*100)+\"%\")\n",
    "print(\"Percentage of reviews with neutral sentiment is \"+str(neu/train_sentences.shape[0]*100)+\"%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before being fed into the LSTM model, the data needs to be padded and tokenized:\n",
    "#### Tokenizing: Kerasâ€™ inbuilt tokenizer API has fit the dataset, which splits the sentences into words and creates a dictionary of all unique words found and their uniquely assigned integers. Each sentence is converted into an array of integers representing all the individual words present in it.\n",
    "#### Sequence Padding: The array representing each sentence in the dataset is filled with zeroes to the left to make the size of the array ten and bring all collections to the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters of the model\n",
    "vocab_size = 3000 # choose based on statistics\n",
    "oov_tok = ''\n",
    "embedding_dim = 100\n",
    "max_length = 200 # choose based on statistics, for example 150 to 200\n",
    "padding_type='post'\n",
    "trunc_type='post'\n",
    "# tokenize sentences\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "# convert train dataset to sequence and pad sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "train_padded = pad_sequences(train_sequences, padding='post', maxlen=max_length)\n",
    "# convert Test dataset to sequence and pad sequences\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_padded = pad_sequences(test_sequences, padding='post', maxlen=max_length)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An embedding layer of dimension 100 converts each word in the sentence into a fixed-length dense vector of size 100. The input dimension is set as the vocabulary size, and the output dimension is 100. Each word in the input will hence get represented by a vector of size 100.\n",
    "A bidirectional LSTM layer of 64 units.\n",
    "A dense (fully connected) layer of 24 units with relu activation.\n",
    "A dense layer of 1 unit and sigmoid activation outputs the probability of the review is positive, i.e. if the label is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_8 (Embedding)     (None, 200, 100)          300000    \n",
      "                                                                 \n",
      " bidirectional_8 (Bidirecti  (None, 128)               84480     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 24)                3096      \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 25        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 387601 (1.48 MB)\n",
      "Trainable params: 387601 (1.48 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model initialization\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(64)),\n",
    "    keras.layers.Dense(24, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "# compile model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "# model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, ..., -1, -1,  0], dtype=int64)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = train_sentences['Sentiment'].values\n",
    "test_labels = test_sentences['Sentiment'].values\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 344ms/step - loss: -0.3725 - accuracy: 0.0000e+00 - val_loss: -0.6902 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 185ms/step - loss: -0.7019 - accuracy: 0.0000e+00 - val_loss: -1.1252 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 176ms/step - loss: -1.1382 - accuracy: 0.0000e+00 - val_loss: -1.6942 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 171ms/step - loss: -1.7084 - accuracy: 0.0000e+00 - val_loss: -2.3949 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 158ms/step - loss: -2.4104 - accuracy: 0.0000e+00 - val_loss: -3.1856 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 168ms/step - loss: -3.2025 - accuracy: 0.0000e+00 - val_loss: -4.0024 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 163ms/step - loss: -4.0208 - accuracy: 0.0000e+00 - val_loss: -4.7918 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 165ms/step - loss: -4.8119 - accuracy: 0.0000e+00 - val_loss: -5.5261 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 163ms/step - loss: -5.5481 - accuracy: 0.0000e+00 - val_loss: -6.2007 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 162ms/step - loss: -6.2248 - accuracy: 0.0000e+00 - val_loss: -6.8249 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 167ms/step - loss: -6.8516 - accuracy: 0.0000e+00 - val_loss: -7.4133 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 167ms/step - loss: -7.4430 - accuracy: 0.0000e+00 - val_loss: -7.9797 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 157ms/step - loss: -8.0129 - accuracy: 0.0000e+00 - val_loss: -8.5350 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 191ms/step - loss: -8.5721 - accuracy: 0.0000e+00 - val_loss: -9.0879 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 202ms/step - loss: -9.1292 - accuracy: 0.0000e+00 - val_loss: -9.6445 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 204ms/step - loss: -9.6905 - accuracy: 0.0000e+00 - val_loss: -10.2097 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 200ms/step - loss: -10.2603 - accuracy: 0.0000e+00 - val_loss: -10.7978 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 192ms/step - loss: -10.8524 - accuracy: 0.0000e+00 - val_loss: -11.4007 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 181ms/step - loss: -11.4609 - accuracy: 0.0000e+00 - val_loss: -12.0202 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 163ms/step - loss: -12.0859 - accuracy: 0.0000e+00 - val_loss: -12.6561 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 164ms/step - loss: -12.7275 - accuracy: 0.0000e+00 - val_loss: -13.3081 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 159ms/step - loss: -13.3853 - accuracy: 0.0000e+00 - val_loss: -13.9759 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 169ms/step - loss: -14.0590 - accuracy: 0.0000e+00 - val_loss: -14.6591 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 166ms/step - loss: -14.7480 - accuracy: 0.0000e+00 - val_loss: -15.3570 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 180ms/step - loss: -15.4517 - accuracy: 0.0000e+00 - val_loss: -16.0687 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 291ms/step - loss: -16.1690 - accuracy: 0.0000e+00 - val_loss: -16.7932 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 221ms/step - loss: -16.8989 - accuracy: 0.0000e+00 - val_loss: -17.5291 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 192ms/step - loss: -17.6399 - accuracy: 0.0000e+00 - val_loss: -18.2751 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 167ms/step - loss: -18.3908 - accuracy: 0.0000e+00 - val_loss: -19.0296 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 169ms/step - loss: -19.1499 - accuracy: 0.0000e+00 - val_loss: -19.7910 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 169ms/step - loss: -19.9156 - accuracy: 0.0000e+00 - val_loss: -20.5581 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 180ms/step - loss: -20.6866 - accuracy: 0.0000e+00 - val_loss: -21.3296 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 179ms/step - loss: -21.4616 - accuracy: 0.0000e+00 - val_loss: -22.1046 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 178ms/step - loss: -22.2397 - accuracy: 0.0000e+00 - val_loss: -22.8822 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 168ms/step - loss: -23.0202 - accuracy: 0.0000e+00 - val_loss: -23.6622 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 171ms/step - loss: -23.8026 - accuracy: 0.0000e+00 - val_loss: -24.4441 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 171ms/step - loss: -24.5865 - accuracy: 0.0000e+00 - val_loss: -25.2277 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 248ms/step - loss: -25.3719 - accuracy: 0.0000e+00 - val_loss: -26.0128 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 187ms/step - loss: -26.1585 - accuracy: 0.0000e+00 - val_loss: -26.7993 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 187ms/step - loss: -26.9463 - accuracy: 0.0000e+00 - val_loss: -27.5872 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 195ms/step - loss: -27.7352 - accuracy: 0.0000e+00 - val_loss: -28.3764 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 192ms/step - loss: -28.5253 - accuracy: 0.0000e+00 - val_loss: -29.1672 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 202ms/step - loss: -29.3168 - accuracy: 0.0000e+00 - val_loss: -29.9597 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 195ms/step - loss: -30.1099 - accuracy: 0.0000e+00 - val_loss: -30.7546 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 210ms/step - loss: -30.9053 - accuracy: 0.0000e+00 - val_loss: -31.5523 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 199ms/step - loss: -31.7034 - accuracy: 0.0000e+00 - val_loss: -32.3535 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 169ms/step - loss: -32.5049 - accuracy: 0.0000e+00 - val_loss: -33.1590 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 254ms/step - loss: -33.3106 - accuracy: 0.0000e+00 - val_loss: -33.9691 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 181ms/step - loss: -34.1208 - accuracy: 0.0000e+00 - val_loss: -34.7842 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 172ms/step - loss: -34.9360 - accuracy: 0.0000e+00 - val_loss: -35.6043 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 182ms/step - loss: -35.7561 - accuracy: 0.0000e+00 - val_loss: -36.4289 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 190ms/step - loss: -36.5807 - accuracy: 0.0000e+00 - val_loss: -37.2575 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 204ms/step - loss: -37.4092 - accuracy: 0.0000e+00 - val_loss: -38.0890 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 187ms/step - loss: -38.2406 - accuracy: 0.0000e+00 - val_loss: -38.9226 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 170ms/step - loss: -39.0740 - accuracy: 0.0000e+00 - val_loss: -39.7574 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 171ms/step - loss: -39.9087 - accuracy: 0.0000e+00 - val_loss: -40.5931 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 176ms/step - loss: -40.7442 - accuracy: 0.0000e+00 - val_loss: -41.4296 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 172ms/step - loss: -41.5805 - accuracy: 0.0000e+00 - val_loss: -42.2673 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 173ms/step - loss: -42.4180 - accuracy: 0.0000e+00 - val_loss: -43.1060 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 169ms/step - loss: -43.2569 - accuracy: 0.0000e+00 - val_loss: -43.9473 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 168ms/step - loss: -44.0976 - accuracy: 0.0000e+00 - val_loss: -44.7904 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 258ms/step - loss: -44.9405 - accuracy: 0.0000e+00 - val_loss: -45.6358 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 182ms/step - loss: -45.7858 - accuracy: 0.0000e+00 - val_loss: -46.4838 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 194ms/step - loss: -46.6336 - accuracy: 0.0000e+00 - val_loss: -47.3343 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 176ms/step - loss: -47.4840 - accuracy: 0.0000e+00 - val_loss: -48.1875 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 160ms/step - loss: -48.3371 - accuracy: 0.0000e+00 - val_loss: -49.0433 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 184ms/step - loss: -49.1930 - accuracy: 0.0000e+00 - val_loss: -49.9020 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 201ms/step - loss: -50.0516 - accuracy: 0.0000e+00 - val_loss: -50.7634 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 198ms/step - loss: -50.9130 - accuracy: 0.0000e+00 - val_loss: -51.6278 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 196ms/step - loss: -51.7774 - accuracy: 0.0000e+00 - val_loss: -52.4950 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 200ms/step - loss: -52.6447 - accuracy: 0.0000e+00 - val_loss: -53.3653 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 176ms/step - loss: -53.5151 - accuracy: 0.0000e+00 - val_loss: -54.2386 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 166ms/step - loss: -54.3886 - accuracy: 0.0000e+00 - val_loss: -55.1150 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 173ms/step - loss: -55.2652 - accuracy: 0.0000e+00 - val_loss: -55.9947 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 169ms/step - loss: -56.1451 - accuracy: 0.0000e+00 - val_loss: -56.8777 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 276ms/step - loss: -57.0283 - accuracy: 0.0000e+00 - val_loss: -57.7640 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 203ms/step - loss: -57.9149 - accuracy: 0.0000e+00 - val_loss: -58.6538 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 212ms/step - loss: -58.8050 - accuracy: 0.0000e+00 - val_loss: -59.5471 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 213ms/step - loss: -59.6986 - accuracy: 0.0000e+00 - val_loss: -60.4440 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 169ms/step - loss: -60.5958 - accuracy: 0.0000e+00 - val_loss: -61.3445 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 173ms/step - loss: -61.4966 - accuracy: 0.0000e+00 - val_loss: -62.2487 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 176ms/step - loss: -62.4012 - accuracy: 0.0000e+00 - val_loss: -63.1566 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 173ms/step - loss: -63.3096 - accuracy: 0.0000e+00 - val_loss: -64.0684 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 174ms/step - loss: -64.2218 - accuracy: 0.0000e+00 - val_loss: -64.9841 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 167ms/step - loss: -65.1378 - accuracy: 0.0000e+00 - val_loss: -65.9036 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 170ms/step - loss: -66.0579 - accuracy: 0.0000e+00 - val_loss: -66.8271 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 170ms/step - loss: -66.9818 - accuracy: 0.0000e+00 - val_loss: -67.7546 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 171ms/step - loss: -67.9099 - accuracy: 0.0000e+00 - val_loss: -68.6862 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 235ms/step - loss: -68.8419 - accuracy: 0.0000e+00 - val_loss: -69.6218 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 177ms/step - loss: -69.7781 - accuracy: 0.0000e+00 - val_loss: -70.5616 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 181ms/step - loss: -70.7184 - accuracy: 0.0000e+00 - val_loss: -71.5056 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 171ms/step - loss: -71.6629 - accuracy: 0.0000e+00 - val_loss: -72.4537 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 189ms/step - loss: -72.6116 - accuracy: 0.0000e+00 - val_loss: -73.4061 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 209ms/step - loss: -73.5645 - accuracy: 0.0000e+00 - val_loss: -74.3627 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 207ms/step - loss: -74.5217 - accuracy: 0.0000e+00 - val_loss: -75.3237 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 199ms/step - loss: -75.4832 - accuracy: 0.0000e+00 - val_loss: -76.2889 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 198ms/step - loss: -76.4491 - accuracy: 0.0000e+00 - val_loss: -77.2586 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 175ms/step - loss: -77.4193 - accuracy: 0.0000e+00 - val_loss: -78.2326 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 178ms/step - loss: -78.3939 - accuracy: 0.0000e+00 - val_loss: -79.2110 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 176ms/step - loss: -79.3729 - accuracy: 0.0000e+00 - val_loss: -80.1938 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "history = model.fit(train_padded, train_labels, \n",
    "                    epochs=num_epochs, verbose=1, \n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000026A0DF0CAF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1341\n",
      "[0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1341, 5]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[128], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(test_labels))\n\u001b[0;32m     12\u001b[0m \u001b[39mprint\u001b[39m(pred_labels)\n\u001b[1;32m---> 13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAccuracy of prediction on test set : \u001b[39m\u001b[39m\"\u001b[39m, accuracy_score(test_labels,pred_labels))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\_param_validation.py:192\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    187\u001b[0m validate_parameter_constraints(\n\u001b[0;32m    188\u001b[0m     parameter_constraints, params, caller_name\u001b[39m=\u001b[39mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\n\u001b[0;32m    189\u001b[0m )\n\u001b[0;32m    191\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 192\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    193\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    194\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    196\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    197\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    198\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[0;32m    199\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    200\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    201\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[0;32m    202\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:221\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \n\u001b[0;32m    157\u001b[0m \u001b[39mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[39m0.5\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[39m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 221\u001b[0m y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m    222\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    223\u001b[0m \u001b[39mif\u001b[39;00m y_type\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mmultilabel\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:86\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     60\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[0;32m     62\u001b[0m \u001b[39m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[39m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m     check_consistent_length(y_true, y_pred)\n\u001b[0;32m     87\u001b[0m     type_true \u001b[39m=\u001b[39m type_of_target(y_true, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     88\u001b[0m     type_pred \u001b[39m=\u001b[39m type_of_target(y_pred, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_pred\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    395\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[0;32m    396\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 397\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    398\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    399\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[0;32m    400\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1341, 5]"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(test_padded)\n",
    "# Get labels based on probability 1 if p>= 0.5 else 0\n",
    "pred_labels = []\n",
    "for i in prediction:\n",
    "    if i >= 0.5:\n",
    "        pred_labels.append(1)\n",
    "    elif (i < 0.5 and i >= -0.5):\n",
    "       pred_labels.append(0)\n",
    "    else:\n",
    "        pred_labels.append(-1)\n",
    "print(len(test_labels))\n",
    "print(pred_labels)\n",
    "print(\"Accuracy of prediction on test set : \", accuracy_score(test_labels,pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
