{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nklsKrmnn/LSC_Sentiment_Analysis/blob/main/Projekt_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#%pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "PTMNkLSmrmSQ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage that what is good for the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId  \\\n",
       "0         1           1   \n",
       "1         2           1   \n",
       "2         3           1   \n",
       "3         4           1   \n",
       "4         5           1   \n",
       "\n",
       "                                                                                                                                                                                         Phrase  \\\n",
       "0  A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .   \n",
       "1                                                                                                                 A series of escapades demonstrating the adage that what is good for the goose   \n",
       "2                                                                                                                                                                                      A series   \n",
       "3                                                                                                                                                                                             A   \n",
       "4                                                                                                                                                                                        series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = pd.read_csv (\"train.tsv\", sep = '\\t')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "train_set.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156060 entries, 0 to 156059\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   PhraseId    156060 non-null  int64 \n",
      " 1   SentenceId  156060 non-null  int64 \n",
      " 2   Phrase      156060 non-null  object\n",
      " 3   Sentiment   156060 non-null  int64 \n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 4.8+ MB\n"
     ]
    }
   ],
   "source": [
    "train_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>156060.000000</td>\n",
       "      <td>156060.000000</td>\n",
       "      <td>156060.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>78030.500000</td>\n",
       "      <td>4079.732744</td>\n",
       "      <td>2.063578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>45050.785842</td>\n",
       "      <td>2502.764394</td>\n",
       "      <td>0.893832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>39015.750000</td>\n",
       "      <td>1861.750000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>78030.500000</td>\n",
       "      <td>4017.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>117045.250000</td>\n",
       "      <td>6244.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>156060.000000</td>\n",
       "      <td>8544.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            PhraseId     SentenceId      Sentiment\n",
       "count  156060.000000  156060.000000  156060.000000\n",
       "mean    78030.500000    4079.732744       2.063578\n",
       "std     45050.785842    2502.764394       0.893832\n",
       "min         1.000000       1.000000       0.000000\n",
       "25%     39015.750000    1861.750000       2.000000\n",
       "50%     78030.500000    4017.000000       2.000000\n",
       "75%    117045.250000    6244.000000       3.000000\n",
       "max    156060.000000    8544.000000       4.000000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Sentiment', ylabel='count'>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='Sentiment', data=train_set)\n",
    "# --> The classes are not equaly represented. We need to agument or bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>This quiet , introspective and entertaining independent is worth seeking .</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>82</td>\n",
       "      <td>Even fans of Ismail Merchant 's work , I suspect , would have a hard time sitting through this one .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>117</td>\n",
       "      <td>A positively thrilling combination of ethnography and all the intrigue , betrayal , deceit and murder of a Shakespearean tragedy or a juicy soap opera .</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>157</td>\n",
       "      <td>Aggressive self-glorification and a manipulative whitewash .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SentenceId  PhraseId  \\\n",
       "0           1         1   \n",
       "1           2        64   \n",
       "2           3        82   \n",
       "3           4       117   \n",
       "4           5       157   \n",
       "\n",
       "                                                                                                                                                                                         Phrase  \\\n",
       "0  A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .   \n",
       "1                                                                                                                    This quiet , introspective and entertaining independent is worth seeking .   \n",
       "2                                                                                          Even fans of Ismail Merchant 's work , I suspect , would have a hard time sitting through this one .   \n",
       "3                                      A positively thrilling combination of ethnography and all the intrigue , betrayal , deceit and murder of a Shakespearean tragedy or a juicy soap opera .   \n",
       "4                                                                                                                                  Aggressive self-glorification and a manipulative whitewash .   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          4  \n",
       "2          1  \n",
       "3          3  \n",
       "4          1  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_full_sentences = train_set.groupby('SentenceId').first().reset_index()\n",
    "train_set_full_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Sentiment', ylabel='count'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='Sentiment', data=train_set_full_sentences)\n",
    "# --> besser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>This quiet , introspective and entertaining independent is worth seeking .</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>82</td>\n",
       "      <td>Even fans of Ismail Merchant 's work , I suspect , would have a hard time sitting through this one .</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>117</td>\n",
       "      <td>A positively thrilling combination of ethnography and all the intrigue , betrayal , deceit and murder of a Shakespearean tragedy or a juicy soap opera .</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>157</td>\n",
       "      <td>Aggressive self-glorification and a manipulative whitewash .</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SentenceId  PhraseId  \\\n",
       "0           1         1   \n",
       "1           2        64   \n",
       "2           3        82   \n",
       "3           4       117   \n",
       "4           5       157   \n",
       "\n",
       "                                                                                                                                                                                         Phrase  \\\n",
       "0  A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .   \n",
       "1                                                                                                                    This quiet , introspective and entertaining independent is worth seeking .   \n",
       "2                                                                                          Even fans of Ismail Merchant 's work , I suspect , would have a hard time sitting through this one .   \n",
       "3                                      A positively thrilling combination of ethnography and all the intrigue , betrayal , deceit and murder of a Shakespearean tragedy or a juicy soap opera .   \n",
       "4                                                                                                                                  Aggressive self-glorification and a manipulative whitewash .   \n",
       "\n",
       "     0    1    2    3    4  \n",
       "0  0.0  1.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  0.0  1.0  \n",
       "2  0.0  1.0  0.0  0.0  0.0  \n",
       "3  0.0  0.0  0.0  1.0  0.0  \n",
       "4  0.0  1.0  0.0  0.0  0.0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_preprocessed = train_set_full_sentences.join(pd.get_dummies(train_set_full_sentences['Sentiment'], dtype=float)).drop('Sentiment', axis=1)\n",
    "data_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sections of config\n",
    "\n",
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 200\n",
    "TRAIN_BATCH_SIZE = 20\n",
    "VALID_BATCH_SIZE = 10\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data(Dataset):\n",
    "\n",
    "    def __init__(self, input, targets, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input = input\n",
    "        self.targets = targets\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.input[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets.iloc[index].values, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4\n",
       "0  0.0  1.0  0.0  0.0  0.0\n",
       "1  0.0  0.0  0.0  0.0  1.0\n",
       "2  0.0  1.0  0.0  0.0  0.0\n",
       "3  0.0  0.0  0.0  1.0  0.0\n",
       "4  0.0  1.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_preprocessed.loc[:, 0:].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (8529, 4)\n",
      "TRAIN Dataset: (6823, 8)\n",
      "TEST Dataset: (1706, 8)\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataset and dataloader for the neural network\n",
    "\n",
    "train_size = 0.8\n",
    "train_dataset=data_preprocessed.sample(frac=train_size,random_state=200)\n",
    "test_dataset=data_preprocessed.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(train_set_full_sentences.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = data(train_dataset['Phrase'], train_dataset.loc[:, 0:], tokenizer, MAX_LEN)\n",
    "testing_set = data(test_dataset['Phrase'], test_dataset.loc[:, 0:], tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BERTClass(\n",
       "  (l1): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (l2): Dropout(p=0.3, inplace=False)\n",
       "  (l3): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
    "\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased', return_dict=False)\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, 5)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output\n",
    "\n",
    "model = BERTClass()\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for _,data in enumerate(training_loader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        if _%5000==0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.7280212640762329\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[46], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(EPOCHS):\n\u001B[1;32m----> 2\u001B[0m     \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[45], line 17\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(epoch)\u001B[0m\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpoch: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Loss:  \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;241m.\u001B[39mitem()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     16\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 17\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     18\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\torch\\_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    486\u001B[0m     )\n\u001B[1;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    195\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    197\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    198\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    199\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 200\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    202\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def validation(epoch):\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[48], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(EPOCHS):\n\u001B[1;32m----> 2\u001B[0m     outputs, targets \u001B[38;5;241m=\u001B[39m \u001B[43mvalidation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(outputs) \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.5\u001B[39m\n\u001B[0;32m      4\u001B[0m     accuracy \u001B[38;5;241m=\u001B[39m metrics\u001B[38;5;241m.\u001B[39maccuracy_score(targets, outputs)\n",
      "Cell \u001B[1;32mIn[47], line 11\u001B[0m, in \u001B[0;36mvalidation\u001B[1;34m(epoch)\u001B[0m\n\u001B[0;32m      9\u001B[0m token_type_ids \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtoken_type_ids\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mlong)\n\u001B[0;32m     10\u001B[0m targets \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtargets\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfloat)\n\u001B[1;32m---> 11\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m fin_targets\u001B[38;5;241m.\u001B[39mextend(targets\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mnumpy()\u001B[38;5;241m.\u001B[39mtolist())\n\u001B[0;32m     13\u001B[0m fin_outputs\u001B[38;5;241m.\u001B[39mextend(torch\u001B[38;5;241m.\u001B[39msigmoid(outputs)\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mnumpy()\u001B[38;5;241m.\u001B[39mtolist())\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn[42], line 11\u001B[0m, in \u001B[0;36mBERTClass.forward\u001B[1;34m(self, ids, mask, token_type_ids)\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, ids, mask, token_type_ids):\n\u001B[1;32m---> 11\u001B[0m     _, output_1\u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43ml1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m     output_2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39ml2(output_1)\n\u001B[0;32m     13\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39ml3(output_2)\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1020\u001B[0m, in \u001B[0;36mBertModel.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m   1011\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[0;32m   1013\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(\n\u001B[0;32m   1014\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[0;32m   1015\u001B[0m     position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1018\u001B[0m     past_key_values_length\u001B[38;5;241m=\u001B[39mpast_key_values_length,\n\u001B[0;32m   1019\u001B[0m )\n\u001B[1;32m-> 1020\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1021\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1022\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1023\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1024\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1025\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1026\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1027\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1028\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1029\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1030\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1031\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1032\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m   1033\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:610\u001B[0m, in \u001B[0;36mBertEncoder.forward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    601\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mcheckpoint\u001B[38;5;241m.\u001B[39mcheckpoint(\n\u001B[0;32m    602\u001B[0m         create_custom_forward(layer_module),\n\u001B[0;32m    603\u001B[0m         hidden_states,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    607\u001B[0m         encoder_attention_mask,\n\u001B[0;32m    608\u001B[0m     )\n\u001B[0;32m    609\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 610\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    611\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    612\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    613\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    614\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    615\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    616\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    617\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    618\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    620\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    621\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:495\u001B[0m, in \u001B[0;36mBertLayer.forward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[0;32m    483\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[0;32m    484\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    485\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    492\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[0;32m    493\u001B[0m     \u001B[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001B[39;00m\n\u001B[0;32m    494\u001B[0m     self_attn_past_key_value \u001B[38;5;241m=\u001B[39m past_key_value[:\u001B[38;5;241m2\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m past_key_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 495\u001B[0m     self_attention_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    496\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    497\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    498\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    499\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mself_attn_past_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    501\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    502\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m self_attention_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    504\u001B[0m     \u001B[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:425\u001B[0m, in \u001B[0;36mBertAttention.forward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[0;32m    415\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[0;32m    416\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    417\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    423\u001B[0m     output_attentions: Optional[\u001B[38;5;28mbool\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    424\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[1;32m--> 425\u001B[0m     self_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    426\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    427\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    428\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    429\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    430\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    431\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    432\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    433\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    434\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput(self_outputs[\u001B[38;5;241m0\u001B[39m], hidden_states)\n\u001B[0;32m    435\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (attention_output,) \u001B[38;5;241m+\u001B[39m self_outputs[\u001B[38;5;241m1\u001B[39m:]  \u001B[38;5;66;03m# add attentions if we output them\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:353\u001B[0m, in \u001B[0;36mBertSelfAttention.forward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[0;32m    350\u001B[0m     attention_scores \u001B[38;5;241m=\u001B[39m attention_scores \u001B[38;5;241m+\u001B[39m attention_mask\n\u001B[0;32m    352\u001B[0m \u001B[38;5;66;03m# Normalize the attention scores to probabilities.\u001B[39;00m\n\u001B[1;32m--> 353\u001B[0m attention_probs \u001B[38;5;241m=\u001B[39m \u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunctional\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msoftmax\u001B[49m\u001B[43m(\u001B[49m\u001B[43mattention_scores\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    355\u001B[0m \u001B[38;5;66;03m# This is actually dropping out entire tokens to attend to, which might\u001B[39;00m\n\u001B[0;32m    356\u001B[0m \u001B[38;5;66;03m# seem a bit unusual, but is taken from the original Transformer paper.\u001B[39;00m\n\u001B[0;32m    357\u001B[0m attention_probs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(attention_probs)\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\torch\\nn\\functional.py:1843\u001B[0m, in \u001B[0;36msoftmax\u001B[1;34m(input, dim, _stacklevel, dtype)\u001B[0m\n\u001B[0;32m   1841\u001B[0m     dim \u001B[38;5;241m=\u001B[39m _get_softmax_dim(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msoftmax\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mdim(), _stacklevel)\n\u001B[0;32m   1842\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m dtype \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1843\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msoftmax\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdim\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1844\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1845\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msoftmax(dim, dtype\u001B[38;5;241m=\u001B[39mdtype)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    outputs, targets = validation(epoch)\n",
    "    outputs = np.array(outputs) >= 0.5\n",
    "    accuracy = metrics.accuracy_score(targets, outputs)\n",
    "    f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
    "    print(f\"Accuracy Score = {accuracy}\")\n",
    "    print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
    "    print(f\"F1 Score (Macro) = {f1_score_macro}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOTp4sQXnn6qJE9TRHSrYyp",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
