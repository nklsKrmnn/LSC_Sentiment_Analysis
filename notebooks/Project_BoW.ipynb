{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>This quiet , introspective and entertaining in...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>82</td>\n",
       "      <td>Even fans of Ismail Merchant 's work , I suspe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>117</td>\n",
       "      <td>A positively thrilling combination of ethnogra...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>157</td>\n",
       "      <td>Aggressive self-glorification and a manipulati...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SentenceId  PhraseId                                             Phrase  \\\n",
       "0           1         1  A series of escapades demonstrating the adage ...   \n",
       "1           2        64  This quiet , introspective and entertaining in...   \n",
       "2           3        82  Even fans of Ismail Merchant 's work , I suspe...   \n",
       "3           4       117  A positively thrilling combination of ethnogra...   \n",
       "4           5       157  Aggressive self-glorification and a manipulati...   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          4  \n",
       "2          1  \n",
       "3          3  \n",
       "4          1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "data = pd.read_csv (\"train.tsv\", sep = '\\t')\n",
    "\n",
    "# Teilsätze rausschmeißen\n",
    "data = data.groupby('SentenceId').first().reset_index()\n",
    "data.head()\n",
    "\n",
    "\n",
    "# so Daten laden, wenn das test_set Sentiment Labels hätte:\n",
    "#train_set = pd.read_csv (\"train.tsv\", sep = '\\t')\n",
    "#test_set= pd.read_csv (\"test.tsv\", sep = '\\t')\n",
    "#pd.set_option('display.max_colwidth', None)\n",
    "#test_set.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    2321\n",
       "1    2200\n",
       "2    1655\n",
       "4    1281\n",
       "0    1072\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Sentiment.value_counts()\n",
    "\n",
    "# Daten ausbalancieren im Trainings-Set? \n",
    "# Jeder Sentiment Value sollte gleiche Anzahl an Samples haben "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# pip install nltk\n",
    "import re\n",
    "\n",
    "\n",
    "# Für Sentimentanalyse zählen nur Wörter  \n",
    "def keep_only_letters(text):\n",
    "    text=re.sub(r'[^a-zA-Z\\s]','',text)\n",
    "    return text\n",
    " \n",
    "# Groß- und Kleinschreibung egal \n",
    "def convert_to_lowercase(text):\n",
    "    return text.lower()\n",
    " \n",
    "def clean_reviews(text):\n",
    "    text = keep_only_letters(text)\n",
    "    text = convert_to_lowercase(text)\n",
    "    return text\n",
    "\n",
    "data['Phrase'] = data['Phrase'].apply(clean_reviews)\n",
    "# train_set['Phrase'] = train_set['Phrase'].apply(clean_reviews)\n",
    "# test_set['Phrase'] = test_set['Phrase'].apply(clean_reviews)\n",
    "\n",
    "# Stop Words definition\n",
    "english_stop_words = nltk.corpus.stopwords.words('english')\n",
    "print(len(english_stop_words))\n",
    "print (english_stop_words[:20])\n",
    "\n",
    "# Stop Words removal\n",
    "def remove_stop_words(text):\n",
    "    for stopword in english_stop_words:\n",
    "        stopword = ' ' + stopword + ' '\n",
    "        text = text.replace(stopword, ' ')\n",
    "    return text\n",
    " \n",
    "\n",
    "data['Phrase'] = data['Phrase'].apply(remove_stop_words) \n",
    "#train_set['Phrase'] = train_set['Phrase'].apply(remove_stop_words)\n",
    "#test_set['Phrase'] = test_set['Phrase'].apply(remove_stop_words)\n",
    "\n",
    "\n",
    "# Stemming\n",
    "def text_stemming(text):\n",
    "    stemmer = nltk.porter.PorterStemmer()\n",
    "    stemmed = ' '.join([stemmer.stem(token) for token in text.split()])\n",
    "    return stemmed\n",
    "\n",
    "data['Phrase'] = data['Phrase'].apply(text_stemming) \n",
    "#train_set['Phrase'] = train_set['Phrase'].apply(text_stemming)\n",
    "#test_set['Phrase'] = test_set['Phrase'].apply(text_stemming)\n",
    "\n",
    "#train_set.head(10)\n",
    "#train_set.shape\n",
    "#test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    1849\n",
       "1    1764\n",
       "2    1330\n",
       "4    1024\n",
       "0     856\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(data, test_size=0.2)\n",
    "train_set.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    1849\n",
       "4    1849\n",
       "0    1849\n",
       "1    1849\n",
       "2    1849\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Daten balancieren\n",
    "\n",
    "Sentiment_groups = train_set.Sentiment.value_counts()\n",
    "for i in range(5):\n",
    "  while Sentiment_groups[i] < Sentiment_groups[3]:\n",
    "    \n",
    "    sent_i_rows = train_set.loc[train_set['Sentiment'] == i]\n",
    "    adding_row = sent_i_rows.sample()\n",
    "    train_set = train_set.append(adding_row, ignore_index=True)\n",
    "\n",
    "    Sentiment_groups = train_set.Sentiment.value_counts()\n",
    "\n",
    "train_set.Sentiment.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Vectorization\n",
    "CountVectorizer aus sklearn um bag-of-words Darstellung von unserem Trainings- und Testset zu erhalten\n",
    "naive bag-of-words text vectorization\n",
    "\n",
    "Nur Trainingsdatensatz zur Definition des Vokabulars heranziehen und \n",
    "das gleiche Vokabular zur Darstellung des Test-Datensatzes verwenden\n",
    "-> Vektorizer an Trainingsdaten anpassen und zur Transformation der Testdaten verwenden\n",
    "\n",
    "weighted version of BOW ausprobieren?\n",
    "\n",
    "### N-Grams\n",
    "\n",
    "Unigramme: Alle eindeutigen Wörter in einem Dokument\n",
    "\n",
    "BiGramme: Alle Permutationen von zwei aufeinanderfolgenden Wörtern in einem Dokument\n",
    "\n",
    "TriGrams: Alle Permutationen von drei aufeinanderfolgenden Wörtern in einem Dokument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "# convert text data to numeric\n",
    "\n",
    "# Argument binary=False: Vocabel-Vector mit term-frequency füllen\n",
    "# binary = True: Vocabel-Vector mit Vorhandensein der Token füllen (1 vorhanden, 0 nicht vorhanden) \n",
    "# ngram _range = Unigram: (1,1); Bigram: (1,2); Trigram: (1,3)\n",
    "\n",
    "vectorizer_uni = sklearn.feature_extraction.text.CountVectorizer(binary=False,ngram_range=(1,1))\n",
    "vectorizer_bi = sklearn.feature_extraction.text.CountVectorizer(binary=False,ngram_range=(1,2))\n",
    "vectorizer_tri = sklearn.feature_extraction.text.CountVectorizer(binary=False,ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_features_train = vectorizer_uni.fit_transform(train_set['Phrase'])\n",
    "uni_features_test = vectorizer_uni.transform(test_set['Phrase'])\n",
    "#print (uni_features_train.shape, uni_features_test.shape)\n",
    "\n",
    "bi_features_train = vectorizer_bi.fit_transform(train_set['Phrase'])\n",
    "bi_features_test = vectorizer_bi.transform(test_set['Phrase'])\n",
    "#print (bi_features_train.shape, bi_features_test.shape)\n",
    "\n",
    "tri_features_train = vectorizer_tri.fit_transform(train_set['Phrase'])\n",
    "tri_features_test = vectorizer_tri.transform(test_set['Phrase'])\n",
    "#print (tri_features_train.shape, tri_features_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigram:\n",
    "(156060, 10998) (66292, 10998): 10998 einzigartige englische token in unserem Vokabular (abgeleitet aus Trainingsdatensatz)\n",
    "Jeder Token wird durch eine Spalte im Datensatz repräsentiert\n",
    "Für jedes Review im Datensatz wird die Frequency der Token (term-frequency) durch Vokabel-Vector der Größe 10998 dargestellt.\n",
    "= Daher haben wir 156060 solcher Vektoren in unserem Trainings-Datensatz und 66292 in unserem Test-Datensatz = Anzahl der Reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_set['Sentiment']\n",
    "test_labels = test_set['Sentiment']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Klassifizierungsmodelle trainieren\n",
    "### 4.1.  Naive Bayes\n",
    "#### 4.1.1. Unigram (Logistic Regression classifier on unigram features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.38      0.34       216\n",
      "           1       0.41      0.42      0.41       436\n",
      "           2       0.27      0.19      0.23       325\n",
      "           3       0.43      0.39      0.41       472\n",
      "           4       0.39      0.49      0.43       257\n",
      "\n",
      "    accuracy                           0.37      1706\n",
      "   macro avg       0.36      0.37      0.36      1706\n",
      "weighted avg       0.37      0.37      0.37      1706\n",
      "\n",
      "[[ 82  81  23  19  11]\n",
      " [ 98 182  63  56  37]\n",
      " [ 59  95  63  79  29]\n",
      " [ 29  70  70 186 117]\n",
      " [  5  21  16  90 125]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    " \n",
    "vectorizer_uni = sklearn.feature_extraction.text.CountVectorizer(binary=False,ngram_range=(1,1))\n",
    "uni_features_train = vectorizer_uni.fit_transform(train_set['Phrase'])\n",
    "uni_features_test = vectorizer_uni.transform(test_set['Phrase'])\n",
    "\n",
    "uni_nb = MultinomialNB()\n",
    "uni_nb.fit(uni_features_train, train_labels)\n",
    " \n",
    "predictions = uni_nb.predict(uni_features_test)\n",
    "print(sklearn.metrics.classification_report(test_labels, predictions, target_names=['0','1','2','3','4']))\n",
    "print(sklearn.metrics.confusion_matrix(test_labels, predictions, labels=[0, 1, 2, 3, 4]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 Naive Bayes: Unigram + Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.35      0.31       216\n",
      "           1       0.40      0.41      0.41       436\n",
      "           2       0.30      0.20      0.24       325\n",
      "           3       0.43      0.42      0.42       472\n",
      "           4       0.38      0.46      0.42       257\n",
      "\n",
      "    accuracy                           0.37      1706\n",
      "   macro avg       0.36      0.37      0.36      1706\n",
      "weighted avg       0.37      0.37      0.37      1706\n",
      "\n",
      "[[ 75  85  25  22   9]\n",
      " [103 180  54  61  38]\n",
      " [ 52  95  65  85  28]\n",
      " [ 28  69  61 196 118]\n",
      " [  8  21  14  96 118]]\n"
     ]
    }
   ],
   "source": [
    "#vectorizer_bi = sklearn.feature_extraction.text.CountVectorizer(binary=False,ngram_range=(1,2))\n",
    "#bi_features_train = vectorizer_bi.fit_transform(train_set['Phrase'])\n",
    "#bi_features_test = vectorizer_bi.transform(test_set['Phrase'])\n",
    "\n",
    "bi_nb = MultinomialNB()\n",
    "bi_nb.fit(bi_features_train, train_labels)\n",
    " \n",
    "predictions = bi_nb.predict(bi_features_test)\n",
    "print(sklearn.metrics.classification_report(test_labels, predictions, target_names=['0','1','2','3','4']))\n",
    "print(sklearn.metrics.confusion_matrix(test_labels, predictions, labels=[0, 1, 2, 3, 4]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.3 Naive Bayes: Unigram + Bigram + Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.35      0.31       216\n",
      "           1       0.40      0.42      0.41       436\n",
      "           2       0.31      0.20      0.25       325\n",
      "           3       0.43      0.42      0.42       472\n",
      "           4       0.37      0.46      0.41       257\n",
      "\n",
      "    accuracy                           0.37      1706\n",
      "   macro avg       0.36      0.37      0.36      1706\n",
      "weighted avg       0.37      0.37      0.37      1706\n",
      "\n",
      "[[ 75  87  23  21  10]\n",
      " [101 181  53  64  37]\n",
      " [ 53  93  66  83  30]\n",
      " [ 27  70  56 198 121]\n",
      " [  8  21  13  98 117]]\n"
     ]
    }
   ],
   "source": [
    "tri_nb = MultinomialNB()\n",
    "tri_nb.fit(tri_features_train, train_labels)\n",
    " \n",
    "predictions = tri_nb.predict(tri_features_test)\n",
    "print(sklearn.metrics.classification_report(test_labels, predictions, target_names=['0','1','2','3','4']))\n",
    "print(sklearn.metrics.confusion_matrix(test_labels, predictions, labels=[0, 1, 2, 3, 4]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Logistische Regression\n",
    "#### 4.2.1. Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.33      0.33       216\n",
      "           1       0.40      0.41      0.40       436\n",
      "           2       0.30      0.30      0.30       325\n",
      "           3       0.41      0.39      0.40       472\n",
      "           4       0.37      0.37      0.37       257\n",
      "\n",
      "    accuracy                           0.37      1706\n",
      "   macro avg       0.36      0.36      0.36      1706\n",
      "weighted avg       0.37      0.37      0.37      1706\n",
      "\n",
      "[[ 71  85  27  24   9]\n",
      " [ 80 180  82  64  30]\n",
      " [ 37  92  99  69  28]\n",
      " [ 25  75  95 182  95]\n",
      " [  7  21  31 103  95]]\n"
     ]
    }
   ],
   "source": [
    "uni_logreg = sklearn.linear_model.LogisticRegression(max_iter=10000)\n",
    "uni_logreg.fit(uni_features_train, train_labels)\n",
    "#print (uni_logreg)\n",
    "\n",
    "\"\"\" default state of the classifier: \n",
    "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
    "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
    "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
    "                   warm_start=False)\"\"\"\n",
    "\n",
    "# Predictions für unseren Test-Datensatz, Accuracy, Confusion-Matrix berechnen: \n",
    "\n",
    "predictions = uni_logreg.predict(uni_features_test)\n",
    "print(sklearn.metrics.classification_report(test_labels, predictions, target_names=['0','1','2','3','4']))\n",
    "print(sklearn.metrics.confusion_matrix(test_labels, predictions, labels=[0, 1, 2, 3, 4]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2. Log Regression: Unigrams + Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.29      0.33       216\n",
      "           1       0.40      0.46      0.43       436\n",
      "           2       0.32      0.31      0.32       325\n",
      "           3       0.41      0.44      0.43       472\n",
      "           4       0.39      0.34      0.36       257\n",
      "\n",
      "    accuracy                           0.39      1706\n",
      "   macro avg       0.38      0.37      0.37      1706\n",
      "weighted avg       0.39      0.39      0.38      1706\n",
      "\n",
      "[[ 62  97  30  20   7]\n",
      " [ 56 201  78  77  24]\n",
      " [ 24  99 100  83  19]\n",
      " [ 12  85  76 209  90]\n",
      " [  4  24  24 117  88]]\n"
     ]
    }
   ],
   "source": [
    "bi_logreg = sklearn.linear_model.LogisticRegression(max_iter=10000)\n",
    "bi_logreg.fit(bi_features_train, train_labels)\n",
    "#print (bi_logreg)\n",
    "\n",
    "predictions = bi_logreg.predict(bi_features_test)\n",
    "print(sklearn.metrics.classification_report(test_labels, predictions, target_names=['0','1','2','3','4']))\n",
    "print(sklearn.metrics.confusion_matrix(test_labels, predictions, labels=[0, 1, 2, 3, 4]))\n",
    "# Feature-Set Größe erhöht sich weil auch Bi-Gramme berücksichtigt "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3. Log Regression: Unigrams + Bigrams + Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.27      0.33       216\n",
      "           1       0.39      0.47      0.43       436\n",
      "           2       0.31      0.30      0.31       325\n",
      "           3       0.42      0.46      0.44       472\n",
      "           4       0.39      0.33      0.36       257\n",
      "\n",
      "    accuracy                           0.39      1706\n",
      "   macro avg       0.39      0.36      0.37      1706\n",
      "weighted avg       0.39      0.39      0.38      1706\n",
      "\n",
      "[[ 58  98  32  21   7]\n",
      " [ 47 203  85  79  22]\n",
      " [ 21 106  97  85  16]\n",
      " [  9  86  72 217  88]\n",
      " [  3  26  24 119  85]]\n"
     ]
    }
   ],
   "source": [
    "tri_logreg = sklearn.linear_model.LogisticRegression(max_iter=10000)\n",
    "tri_logreg.fit(tri_features_train, train_labels)\n",
    "#print (tri_logreg)\n",
    "\n",
    "predictions = tri_logreg.predict(tri_features_test)\n",
    "print(sklearn.metrics.classification_report(test_labels, predictions, target_names=['0','1','2','3','4']))\n",
    "print(sklearn.metrics.confusion_matrix(test_labels, predictions, labels=[0, 1, 2, 3, 4]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Linear Support Vector Machine (LSVM) \n",
    "#### 4.3.1. Unigram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.31      0.30       216\n",
      "           1       0.36      0.36      0.36       436\n",
      "           2       0.28      0.27      0.27       325\n",
      "           3       0.37      0.36      0.36       472\n",
      "           4       0.35      0.37      0.36       257\n",
      "\n",
      "    accuracy                           0.34      1706\n",
      "   macro avg       0.33      0.34      0.33      1706\n",
      "weighted avg       0.34      0.34      0.34      1706\n",
      "\n",
      "[[ 68  86  25  27  10]\n",
      " [ 82 159  90  80  25]\n",
      " [ 39  95  88  72  31]\n",
      " [ 34  69  90 168 111]\n",
      " [  9  27  24 102  95]]\n"
     ]
    }
   ],
   "source": [
    "uni_lsvm = sklearn.svm.LinearSVC()\n",
    "uni_lsvm.fit(uni_features_train, train_labels)\n",
    " \n",
    "predictions = uni_lsvm.predict(uni_features_test)\n",
    "print(sklearn.metrics.classification_report(test_labels, predictions, target_names=['0','1','2','3','4']))\n",
    "print(sklearn.metrics.confusion_matrix(test_labels, predictions, labels=[0, 1, 2, 3, 4]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2. LSVM: Unigram + Bigram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.28      0.31       216\n",
      "           1       0.38      0.43      0.40       436\n",
      "           2       0.29      0.29      0.29       325\n",
      "           3       0.42      0.42      0.42       472\n",
      "           4       0.35      0.33      0.34       257\n",
      "\n",
      "    accuracy                           0.37      1706\n",
      "   macro avg       0.36      0.35      0.35      1706\n",
      "weighted avg       0.37      0.37      0.36      1706\n",
      "\n",
      "[[ 60  99  34  14   9]\n",
      " [ 58 187  91  71  29]\n",
      " [ 35  99  95  77  19]\n",
      " [ 12  79  84 198  99]\n",
      " [  3  31  29 110  84]]\n"
     ]
    }
   ],
   "source": [
    "bi_lsvm = sklearn.svm.LinearSVC()\n",
    "bi_lsvm.fit(bi_features_train, train_labels)\n",
    " \n",
    "predictions = bi_lsvm.predict(bi_features_test)\n",
    "print(sklearn.metrics.classification_report(test_labels, predictions, target_names=['0','1','2','3','4']))\n",
    "print(sklearn.metrics.confusion_matrix(test_labels, predictions, labels=[0, 1, 2, 3, 4]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.3. LSVM: Unigram + Bigram + Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.25      0.31       216\n",
      "           1       0.38      0.44      0.41       436\n",
      "           2       0.30      0.32      0.31       325\n",
      "           3       0.43      0.44      0.43       472\n",
      "           4       0.37      0.34      0.35       257\n",
      "\n",
      "    accuracy                           0.38      1706\n",
      "   macro avg       0.37      0.36      0.36      1706\n",
      "weighted avg       0.38      0.38      0.37      1706\n",
      "\n",
      "[[ 55 101  35  18   7]\n",
      " [ 49 192  97  67  31]\n",
      " [ 20 101 103  79  22]\n",
      " [ 13  78  84 206  91]\n",
      " [  3  29  26 112  87]]\n"
     ]
    }
   ],
   "source": [
    "tri_lsvm = sklearn.svm.LinearSVC()\n",
    "tri_lsvm.fit(tri_features_train, train_labels)\n",
    " \n",
    "predictions = tri_lsvm.predict(tri_features_test)\n",
    "print(sklearn.metrics.classification_report(test_labels, predictions, target_names=['0','1','2','3','4']))\n",
    "print(sklearn.metrics.confusion_matrix(test_labels, predictions, labels=[0, 1, 2, 3, 4]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
